#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ–°é—»çŠ¶æ€æ æœåŠ¡ - é›†æˆæ–°é—»æŠ“å–ã€HTTP APIå’ŒçŠ¶æ€æ æ˜¾ç¤º
ç§»é™¤æœªä½¿ç”¨çš„ç¿»è¯‘åŠŸèƒ½ï¼Œä¿æŒæ ¸å¿ƒæ–°é—»èšåˆå’Œæ˜¾ç¤ºåŠŸèƒ½
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import json
import threading
import logging
import signal
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from http.server import HTTPServer, BaseHTTPRequestHandler
import urllib.parse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/news_service.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class NewsItem:
    """æ–°é—»é¡¹æ•°æ®ç»“æ„"""
    def __init__(self, title: str, url: str, source: str = "", news_time: Optional[str] = None, stock_info: Optional[str] = None):
        self.title = title
        self.url = url
        self.source = source
        self.news_time = news_time  # æ–°é—»å®é™…å‘å¸ƒæ—¶é—´
        self.stock_info = stock_info  # ç›¸å…³è‚¡ç¥¨ä¿¡æ¯
        self.timestamp = datetime.now()  # æŠ“å–æ—¶é—´
        self.id = f"{source}_{hash(title)}_{int(self.timestamp.timestamp())}"
    
    def to_dict(self) -> Dict:
        result = {
            'id': self.id,
            'title': self.title,
            'url': self.url,
            'source': self.source,
            'timestamp': self.timestamp.isoformat()
        }
        
        # æ·»åŠ å¯é€‰å­—æ®µ
        if self.news_time:
            result['news_time'] = self.news_time
        if self.stock_info:
            result['stock_info'] = self.stock_info
            
        return result

class NewsPool:
    """æ–°é—»æ± ç®¡ç†å™¨"""
    def __init__(self, max_size: int = 100, refresh_interval: int = 60):
        self.news_items: List[NewsItem] = []
        self.max_size = max_size
        self.refresh_interval = refresh_interval
        self.lock = threading.Lock()
        self.last_refresh = None
        self.refresh_thread = None
        self.running = True
        
        # åŠ è½½æ–°é—»æºé…ç½®
        self.load_news_sources_config()
        
        # å¯åŠ¨è‡ªåŠ¨åˆ·æ–°
        self.start_auto_refresh()
        
        # ç«‹å³è·å–ä¸€æ¬¡æ–°é—»
        self.refresh_news()
    
    def load_news_sources_config(self):
        """åŠ è½½æ–°é—»æºé…ç½®ï¼Œæ”¯æŒå¯å¼€å…³é…ç½®"""
        # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
        config_file = os.path.expanduser('~/.claude/news_sources_config.json')
        
        # é»˜è®¤æ–°é—»æºé…ç½®ï¼ˆæ‰€æœ‰æºé»˜è®¤å¯ç”¨ï¼‰
        default_config = {
            'sources': {
                '36kr': {
                    'enabled': True,
                    'name': '36kr',
                    'url': 'https://36kr.com',
                    'selectors': [
                        'a[href*="/p/"]',
                        'a.item-title',
                        '.article-title',
                        'h3.title a'
                    ],
                    'icon': 'ğŸ’¼'
                },
                'techcrunch': {
                    'enabled': True,
                    'name': 'TechCrunch',
                    'url': 'https://techcrunch.com',
                    'selectors': [
                        'h2.post-block__title a',
                        '.post-title a',
                        'h3 a',
                        '.article-title'
                    ],
                    'icon': 'ğŸš€'
                },
                'huxiu': {
                    'enabled': True,
                    'name': 'è™å—…',
                    'url': 'https://www.huxiu.com',
                    'selectors': [
                        '.article-title',
                        'h3.title a',
                        '.news-title',
                        'a.item-title'
                    ],
                    'icon': 'ğŸ¦†'
                },
                'tmtpost': {
                    'enabled': True,
                    'name': 'é’›åª’ä½“',
                    'url': 'https://www.tmtpost.com',
                    'selectors': [
                        'a._tit',
                        '.item ._tit',
                        'h3 a',
                        '.article-title'
                    ],
                    'icon': 'ğŸ”§'
                },
                'leiphone': {
                    'enabled': True,
                    'name': 'é›·é”‹ç½‘',
                    'url': 'https://www.leiphone.com',
                    'selectors': [
                        'h3 a',
                        'a[href*="news"]',
                        '.article-title',
                        '.news-title'
                    ],
                    'icon': 'âš¡'
                },
                'cls_telegraph': {
                    'enabled': True,
                    'name': 'è´¢è”ç¤¾ç”µæŠ¥',
                    'url': 'https://www.cls.cn/telegraph',
                    'selectors': [
                        '.telegraph-content a',
                        'a[href*="/telegraph/"]',
                        '.telegraph-item a',
                        'a'
                    ],
                    'icon': 'ğŸ“ˆ'
                },
                'cls_finance': {
                    'enabled': True,
                    'name': 'è´¢è”ç¤¾ç›˜é¢',
                    'url': 'https://www.cls.cn/subject/1103',
                    'selectors': [
                        'a[href*="/detail/"]',
                        '.article-item a',
                        '.news-item a',
                        'a'
                    ],
                    'icon': 'ğŸ’¹'
                },
                'cls_depth': {
                    'enabled': True,
                    'name': 'è´¢è”ç¤¾æ·±åº¦',
                    'url': 'https://www.cls.cn/depth?id=1000',
                    'selectors': [
                        'a[href*="/depth/"]',
                        '.depth-item a',
                        'h3 a',
                        'a'
                    ],
                    'icon': 'ğŸ“Š'
                }
            }
        }
        
        # å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®
                    for source_key, source_config in default_config['sources'].items():
                        if source_key in user_config.get('sources', {}):
                            # æ›´æ–°ç”¨æˆ·è‡ªå®šä¹‰çš„è®¾ç½®
                            source_config.update(user_config['sources'][source_key])
                    
                    logger.info(f"å·²åŠ è½½ç”¨æˆ·é…ç½®æ–‡ä»¶: {config_file}")
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
                os.makedirs(os.path.dirname(config_file), exist_ok=True)
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, ensure_ascii=False, indent=2)
                logger.info(f"å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_file}")
        except Exception as e:
            logger.warning(f"é…ç½®æ–‡ä»¶å¤„ç†é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # åªå¯ç”¨å·²å¯ç”¨çš„æ–°é—»æº
        self.news_sources = {
            key: config for key, config in default_config['sources'].items()
            if config.get('enabled', True)
        }
        
        enabled_sources = list(self.news_sources.keys())
        logger.info(f"å·²å¯ç”¨çš„æ–°é—»æº: {enabled_sources}")
    
    def start_auto_refresh(self):
        """å¯åŠ¨è‡ªåŠ¨åˆ·æ–°çº¿ç¨‹"""
        if self.refresh_thread is None or not self.refresh_thread.is_alive():
            self.refresh_thread = threading.Thread(target=self._auto_refresh_worker, daemon=True)
            self.refresh_thread.start()
            logger.info("è‡ªåŠ¨åˆ·æ–°çº¿ç¨‹å·²å¯åŠ¨")
    
    def _auto_refresh_worker(self):
        """è‡ªåŠ¨åˆ·æ–°å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                time.sleep(self.refresh_interval)
                if self.running:
                    self.refresh_news()
            except Exception as e:
                logger.error(f"è‡ªåŠ¨åˆ·æ–°é”™è¯¯: {e}")
    
    def refresh_news(self):
        """åˆ·æ–°æ–°é—»æ± """
        logger.info("å¼€å§‹åˆ·æ–°æ–°é—»...")
        
        new_items = []
        for source_key, source_config in self.news_sources.items():
            try:
                items = self._fetch_news_from_source(source_config)
                new_items.extend(items)
                logger.info(f"ä» {source_config['name']} è·å–åˆ° {len(items)} æ¡æ–°é—»")
            except Exception as e:
                logger.error(f"ä» {source_config['name']} è·å–æ–°é—»å¤±è´¥: {e}")
        
        with self.lock:
            # åˆå¹¶æ–°æ—§æ–°é—»ï¼Œå»é‡
            all_items = new_items + self.news_items
            unique_items = []
            seen_titles = set()
            
            for item in all_items:
                if item.title not in seen_titles:
                    seen_titles.add(item.title)
                    unique_items.append(item)
            
            # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            unique_items.sort(key=lambda x: x.timestamp, reverse=True)
            
            # æ¸…ç†è¿‡æœŸå†…å®¹ï¼ˆ6å°æ—¶å‰ï¼‰
            cutoff_time = datetime.now() - timedelta(hours=6)
            fresh_items = [item for item in unique_items if item.timestamp > cutoff_time]
            
            # é™åˆ¶æ± å¤§å°
            self.news_items = fresh_items[:self.max_size]
            self.last_refresh = datetime.now()
            
            logger.info(f"æ–°é—»æ± åˆ·æ–°å®Œæˆï¼Œå½“å‰æœ‰ {len(self.news_items)} æ¡æ–°é—»")
    
    def _fetch_news_from_source(self, source_config: Dict) -> List[NewsItem]:
        """ä»å•ä¸ªæ–°é—»æºè·å–æ–°é—»"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(source_config['url'], headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_items = []
            
            # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
            for selector in source_config['selectors']:
                elements = soup.select(selector)
                if elements:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    break
            
            if not elements:
                logger.warning(f"æœªæ‰¾åˆ°æ–°é—»å…ƒç´ : {source_config['name']}")
                return []
            
            for element in elements[:20]:  # é™åˆ¶æ¯ä¸ªæºæœ€å¤š20æ¡
                try:
                    if element.name == 'a':
                        title = element.get_text(strip=True)
                        url = element.get('href', '')
                    else:
                        link = element.find('a')
                        if link:
                            title = link.get_text(strip=True)
                            url = link.get('href', '')
                        else:
                            title = element.get_text(strip=True)
                            url = ''
                    
                    if title and len(title) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                        # å¤„ç†ç›¸å¯¹URL
                        if url and not url.startswith('http'):
                            if url.startswith('/'):
                                # è·å–åŸºç¡€åŸŸå
                                from urllib.parse import urlparse
                                parsed_url = urlparse(source_config['url'])
                                base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
                                url = f"{base_url}{url}"
                            else:
                                url = f"{source_config['url'].rstrip('/')}/{url}"
                        
                        # å¯¹è´¢è”ç¤¾æ¥æºæå–é¢å¤–ä¿¡æ¯
                        news_time = None
                        stock_info = None
                        
                        if "è´¢è”ç¤¾" in source_config['name'] and url:
                            extracted_data = self._extract_cls_data(url, soup, element)
                            news_time = extracted_data.get('news_time')
                            stock_info = extracted_data.get('stock_info')
                        
                        news_item = NewsItem(title, url, source_config['name'], news_time, stock_info)
                        news_items.append(news_item)
                
                except Exception as e:
                    logger.debug(f"å¤„ç†æ–°é—»é¡¹é”™è¯¯: {e}")
                    continue
            
            return news_items[:20]  # è¿”å›æœ€å¤š20æ¡
            
        except Exception as e:
            logger.error(f"è·å–æ–°é—»å¤±è´¥ {source_config['name']}: {e}")
            return []
    
    def _extract_cls_data(self, url: str, soup: BeautifulSoup, element) -> Dict:
        """æå–è´¢è”ç¤¾ç‰¹å®šæ•°æ®ï¼šæ—¶é—´å’Œè‚¡ç¥¨ä¿¡æ¯"""
        result = {'news_time': None, 'stock_info': None}
        
        try:
            # å¦‚æœæ˜¯è¯¦æƒ…é¡µé¢ï¼Œéœ€è¦è®¿é—®é¡µé¢æå–ä¿¡æ¯
            if '/detail/' in url:
                result = self._fetch_detail_page_data(url)
                
            elif 'telegraph' in url:
                # å¦‚æœæ˜¯ç”µæŠ¥é¡µé¢ï¼Œç›´æ¥ä»å½“å‰é¡µé¢æå–
                result = self._extract_from_telegraph_page(soup, element)
                
        except Exception as e:
            logger.debug(f"æå–CLSæ•°æ®é”™è¯¯: {e}")
        
        return result
    
    def _fetch_detail_page_data(self, url: str) -> Dict:
        """è®¿é—®è¯¦æƒ…é¡µé¢è·å–æ—¶é—´å’Œè‚¡ç¥¨ä¿¡æ¯"""
        result = {'news_time': None, 'stock_info': None}
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            detail_soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ—¶é—´ä¿¡æ¯ - è´¢è”ç¤¾è¯¦æƒ…é¡µçš„æ—¶é—´æ ¼å¼
            # æŸ¥æ‰¾å¯èƒ½çš„æ—¶é—´å…ƒç´ 
            time_selectors = [
                '.time',
                '.publish-time',
                '[class*="time"]',
                '.article-time',
                'time'
            ]
            
            for selector in time_selectors:
                time_element = detail_soup.select_one(selector)
                if time_element:
                    time_text = time_element.get_text(strip=True)
                    if time_text and ('å¹´' in time_text or 'æœˆ' in time_text or ':' in time_text):
                        result['news_time'] = time_text
                        break
            
            # æå–è‚¡ç¥¨ä¿¡æ¯ - æŸ¥æ‰¾è‚¡ç¥¨ç›¸å…³å…ƒç´ 
            stock_selectors = [
                '.industry-stock a',
                '[class*="stock"] a',
                'a[href*="stock"]'
            ]
            
            for selector in stock_selectors:
                stock_elements = detail_soup.select(selector)
                if stock_elements:
                    stock_items = []
                    for elem in stock_elements[:5]:  # æœ€å¤š5åªè‚¡ç¥¨
                        name_span = elem.find('span', class_='c-222') or elem.find('span')
                        change_span = elem.find('span', class_='c-de0422') or elem.find_all('span')[-1] if elem.find_all('span') else None
                        
                        if name_span and change_span:
                            name = name_span.get_text(strip=True)
                            change = change_span.get_text(strip=True)
                            if '+' in change or '-' in change or '%' in change:
                                stock_items.append(f"{name} {change}")
                    
                    if stock_items:
                        result['stock_info'] = ' '.join(stock_items)
                        break
                        
        except Exception as e:
            logger.debug(f"è·å–è¯¦æƒ…é¡µé¢æ•°æ®é”™è¯¯: {e}")
            
        return result
    
    def _extract_from_telegraph_page(self, soup: BeautifulSoup, element) -> Dict:
        """ä»ç”µæŠ¥é¡µé¢æå–æ•°æ®"""
        result = {'news_time': None, 'stock_info': None}
        
        try:
            # æå–æ—¶é—´ä¿¡æ¯
            time_element = soup.find('span', class_='telegraph-time-box')
            if time_element:
                time_text = time_element.get_text(strip=True)
                if time_text and ':' in time_text:
                    # æ ¼å¼åŒ–æ—¶é—´ï¼ˆæ·»åŠ ä»Šå¤©çš„æ—¥æœŸï¼‰
                    today = datetime.now().strftime('%Y-%m-%d')
                    result['news_time'] = f"{today} {time_text}"
            
            # æå–è‚¡ç¥¨ä¿¡æ¯
            stock_container = soup.find('div', class_='industry-stock')
            if stock_container:
                stock_items = []
                stock_links = stock_container.find_all('a')
                for link in stock_links[:5]:  # æœ€å¤š5åªè‚¡ç¥¨
                    name_span = link.find('span', class_='c-222')
                    change_span = link.find('span', class_='c-de0422')
                    if name_span and change_span:
                        name = name_span.get_text(strip=True)
                        change = change_span.get_text(strip=True)
                        stock_items.append(f"{name} {change}")
                
                if stock_items:
                    result['stock_info'] = ' '.join(stock_items)
                    
        except Exception as e:
            logger.debug(f"ä»ç”µæŠ¥é¡µé¢æå–æ•°æ®é”™è¯¯: {e}")
            
        return result
    
    def get_next_news(self) -> Optional[NewsItem]:
        """è·å–ä¸‹ä¸€æ¡æ–°é—»ï¼ˆæ—¶é—´è½®æ’­ï¼‰"""
        with self.lock:
            if not self.news_items:
                return None
            
            # åŸºäºæ—¶é—´çš„ä¼ªéšæœºè½®æ’­
            current_epoch = int(time.time())
            rotation_interval = 5  # 5ç§’è½®æ’­ä¸€æ¬¡
            rotation_index = (current_epoch // rotation_interval) % len(self.news_items)
            
            return self.news_items[rotation_index]
    
    def get_random_news(self, count: int = 5) -> List[NewsItem]:
        """è·å–éšæœºæ–°é—»"""
        with self.lock:
            if not self.news_items:
                return []
            return random.sample(self.news_items, min(count, len(self.news_items)))
    
    def get_status(self) -> Dict:
        """è·å–æœåŠ¡çŠ¶æ€"""
        with self.lock:
            return {
                'total_news': len(self.news_items),
                'last_refresh': self.last_refresh.isoformat() if self.last_refresh else None,
                'sources': list(self.news_sources.keys()),
                'auto_refresh_interval': self.refresh_interval
            }
    
    def stop(self):
        """åœæ­¢æœåŠ¡"""
        self.running = False
        if self.refresh_thread:
            self.refresh_thread.join(timeout=1)

class NewsAPIHandler(BaseHTTPRequestHandler):
    """HTTP APIå¤„ç†å™¨"""
    
    def do_GET(self):
        parsed_path = urllib.parse.urlparse(self.path)
        path = parsed_path.path
        query_params = urllib.parse.parse_qs(parsed_path.query)
        
        try:
            if path == '/status':
                self._handle_status()
            elif path == '/next':
                self._handle_next()
            elif path == '/random':
                count = int(query_params.get('count', ['5'])[0])
                self._handle_random(count)
            elif path == '/refresh':
                self._handle_refresh()
            else:
                self._send_error(404, "Not Found")
        except Exception as e:
            logger.error(f"APIé”™è¯¯: {e}")
            self._send_error(500, str(e))
    
    def _handle_status(self):
        status = news_pool.get_status()
        self._send_json_response(status)
    
    def _handle_next(self):
        news_item = news_pool.get_next_news()
        if news_item:
            self._send_json_response(news_item.to_dict())
        else:
            self._send_json_response({'error': 'No news available'})
    
    def _handle_random(self, count: int):
        news_items = news_pool.get_random_news(count)
        response = [item.to_dict() for item in news_items]
        self._send_json_response(response)
    
    def _handle_refresh(self):
        threading.Thread(target=news_pool.refresh_news, daemon=True).start()
        self._send_json_response({'message': 'Refresh started'})
    
    def _send_json_response(self, data):
        response = json.dumps(data, ensure_ascii=False, indent=2)
        self.send_response(200)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.send_header('Content-Length', str(len(response.encode('utf-8'))))
        self.end_headers()
        self.wfile.write(response.encode('utf-8'))
    
    def _send_error(self, code: int, message: str):
        self.send_response(code)
        self.send_header('Content-Type', 'text/plain; charset=utf-8')
        self.end_headers()
        self.wfile.write(message.encode('utf-8'))
    
    def log_message(self, format, *args):
        # å‡å°‘HTTPæ—¥å¿—è¾“å‡º
        pass

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    logger.info("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æœåŠ¡...")
    global news_pool, httpd
    if news_pool:
        news_pool.stop()
    if httpd:
        httpd.shutdown()
    sys.exit(0)

def check_existing_service():
    """æ£€æŸ¥æ˜¯å¦å·²æœ‰å¥åº·çš„æœåŠ¡åœ¨è¿è¡Œï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡å¯åŠ¨"""
    import subprocess
    try:
        # å…ˆæ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
        result = subprocess.run(['lsof', '-ti:8765'], capture_output=True, text=True, timeout=3)
        if result.returncode == 0 and result.stdout.strip():
            # ç«¯å£è¢«å ç”¨ï¼Œæ£€æŸ¥æœåŠ¡æ˜¯å¦å¥åº·
            try:
                response = requests.get('http://localhost:8765/status', timeout=3)
                if response.status_code == 200:
                    data = response.json()
                    if 'total_news' in data:
                        logger.info(f"æ£€æµ‹åˆ°å¥åº·çš„æ–°é—»æœåŠ¡æ­£åœ¨è¿è¡Œ (æ–°é—»æ•°: {data.get('total_news', 0)})ï¼Œè·³è¿‡å¯åŠ¨")
                        sys.exit(0)
                    else:
                        logger.warning("æœåŠ¡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œéœ€è¦é‡å¯")
                else:
                    logger.warning(f"æœåŠ¡å“åº”å¼‚å¸¸ (çŠ¶æ€ç : {response.status_code})ï¼Œéœ€è¦é‡å¯")
            except Exception as e:
                logger.warning(f"æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {e}ï¼Œéœ€è¦é‡å¯")
            
            # æœåŠ¡ä¸å¥åº·ï¼Œæ¸…ç†è¿›ç¨‹
            pids = result.stdout.strip().split('\n')
            logger.info("æ£€æµ‹åˆ°ç«¯å£è¢«å ç”¨ä½†æœåŠ¡ä¸å¥åº·ï¼Œæ­£åœ¨æ¸…ç†...")
            for pid in pids:
                if pid.strip():
                    try:
                        logger.info(f"æ­£åœ¨åœæ­¢è¿›ç¨‹ PID: {pid}")
                        subprocess.run(['kill', '-9', pid.strip()], timeout=3)
                    except Exception as e:
                        logger.warning(f"åœæ­¢è¿›ç¨‹ {pid} æ—¶å‡ºé”™: {e}")
            
            # ç­‰å¾…ç«¯å£é‡Šæ”¾
            import time
            time.sleep(2)
            logger.info("è¿›ç¨‹å·²æ¸…ç†ï¼Œå‡†å¤‡å¯åŠ¨æ–°æœåŠ¡")
        else:
            logger.info("ç«¯å£8765ç©ºé—²ï¼Œå‡†å¤‡å¯åŠ¨æœåŠ¡")
                
    except Exception as e:
        logger.warning(f"æ£€æŸ¥ç°æœ‰æœåŠ¡æ—¶å‡ºé”™: {e}")
        pass

def main():
    global news_pool, httpd
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœåŠ¡è¿è¡Œ
    check_existing_service()
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        # åˆå§‹åŒ–æ–°é—»æ± 
        logger.info("åˆå§‹åŒ–æ–°é—»æ± ...")
        news_pool = NewsPool()
        
        # å¯åŠ¨HTTPæœåŠ¡å™¨
        port = int(os.getenv('NEWS_SERVICE_PORT', '8765'))
        server_address = ('localhost', port)
        httpd = HTTPServer(server_address, NewsAPIHandler)
        
        logger.info(f"æ–°é—»æœåŠ¡å·²å¯åŠ¨åœ¨ http://{server_address[0]}:{server_address[1]}")
        logger.info("API endpoints:")
        logger.info("  GET /status  - æœåŠ¡çŠ¶æ€")
        logger.info("  GET /next    - ä¸‹ä¸€æ¡æ–°é—»")
        logger.info("  GET /random?count=N - éšæœºæ–°é—»")
        logger.info("  GET /refresh - æ‰‹åŠ¨åˆ·æ–°")
        
        httpd.serve_forever()
        
    except KeyboardInterrupt:
        signal_handler(signal.SIGINT, None)
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    news_pool = None
    httpd = None
    main()