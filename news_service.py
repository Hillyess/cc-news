#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ–°é—»çŠ¶æ€æ æœåŠ¡ - é›†æˆæ–°é—»æŠ“å–ã€HTTP APIå’ŒçŠ¶æ€æ æ˜¾ç¤º
ç§»é™¤æœªä½¿ç”¨çš„ç¿»è¯‘åŠŸèƒ½ï¼Œä¿æŒæ ¸å¿ƒæ–°é—»èšåˆå’Œæ˜¾ç¤ºåŠŸèƒ½
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import json
import threading
import logging
import signal
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Union
from http.server import HTTPServer, BaseHTTPRequestHandler
import urllib.parse
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/news_service.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class NewsItem:
    """æ–°é—»é¡¹æ•°æ®ç»“æ„"""
    def __init__(self, title: str, url: str, source: str = "", news_time: Optional[str] = None, stock_info: Optional[str] = None):
        self.title = title
        self.url = url
        self.source = source
        self.news_time = news_time  # æ–°é—»å®é™…å‘å¸ƒæ—¶é—´
        self.stock_info = stock_info  # ç›¸å…³è‚¡ç¥¨ä¿¡æ¯
        self.timestamp = datetime.now()  # æŠ“å–æ—¶é—´
        self.id = f"{source}_{hash(title)}_{int(self.timestamp.timestamp())}"
    
    def to_dict(self) -> Dict:
        result = {
            'id': self.id,
            'title': self.title,
            'url': self.url,
            'source': self.source,
            'timestamp': self.timestamp.isoformat()
        }
        
        # æ·»åŠ å¯é€‰å­—æ®µ
        if self.news_time:
            result['news_time'] = self.news_time
        if self.stock_info:
            result['stock_info'] = self.stock_info
            
        return result

class StockIndex:
    """è‚¡æŒ‡æ•°æ®ç»“æ„"""
    def __init__(self, name: str, code: str, current_price: float, change: float, change_percent: float):
        self.name = name
        self.code = code 
        self.current_price = current_price
        self.change = change
        self.change_percent = change_percent
        self.timestamp = datetime.now()
    
    def to_dict(self) -> Dict:
        return {
            'name': self.name,
            'code': self.code,
            'current_price': self.current_price,
            'change': self.change,
            'change_percent': self.change_percent,
            'timestamp': self.timestamp.isoformat()
        }

class SectorData:
    """æ¿å—æ•°æ®ç»“æ„"""
    def __init__(self, name: str, change_percent: float, sector_type: str = "gainer"):
        self.name = name
        self.change_percent = change_percent
        self.sector_type = sector_type  # "gainer" or "loser"
        self.timestamp = datetime.now()
    
    def to_dict(self) -> Dict:
        return {
            'name': self.name,
            'change_percent': self.change_percent,
            'sector_type': self.sector_type,
            'timestamp': self.timestamp.isoformat()
        }

class BigAPool:
    """å¤§Aæ¨¡å¼æ•°æ®ç®¡ç†å™¨"""
    def __init__(self):
        self.lock = threading.Lock()
        self.indices: List[StockIndex] = []
        self.sectors: List[SectorData] = []
        self.telegraph_items: List[NewsItem] = []
        self.last_indices_update = datetime.min
        self.last_sectors_update = datetime.min
        self.last_telegraph_update = datetime.min
        
        # æ›´æ–°é—´éš”è®¾ç½®
        self.indices_update_interval = 60  # è‚¡æŒ‡æ¯åˆ†é’Ÿæ›´æ–°
        self.sectors_update_interval = 300  # æ¿å—æ¯5åˆ†é’Ÿæ›´æ–°
        self.telegraph_update_interval = 30  # ç”µæŠ¥æ¯30ç§’æ›´æ–°
        
        # å¯åŠ¨æ•°æ®æ›´æ–°çº¿ç¨‹
        self.running = True
        self.update_thread = threading.Thread(target=self._update_worker, daemon=True)
        self.update_thread.start()
    
    def _update_worker(self):
        """åå°æ•°æ®æ›´æ–°å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                now = datetime.now()
                
                # æ›´æ–°è‚¡æŒ‡æ•°æ®
                if (now - self.last_indices_update).total_seconds() >= self.indices_update_interval:
                    self._update_indices()
                    self.last_indices_update = now
                
                # æ›´æ–°æ¿å—æ•°æ®
                if (now - self.last_sectors_update).total_seconds() >= self.sectors_update_interval:
                    self._update_sectors()
                    self.last_sectors_update = now
                
                # æ›´æ–°ç”µæŠ¥æ•°æ®
                if (now - self.last_telegraph_update).total_seconds() >= self.telegraph_update_interval:
                    self._update_telegraph()
                    self.last_telegraph_update = now
                
                time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"BigAæ•°æ®æ›´æ–°é”™è¯¯: {e}")
    
    def _update_indices(self):
        """æ›´æ–°è‚¡æŒ‡æ•°æ®"""
        try:
            new_indices = self._fetch_stock_indices()
            with self.lock:
                self.indices = new_indices
            logger.info(f"å·²æ›´æ–°{len(new_indices)}ä¸ªè‚¡æŒ‡æ•°æ®")
        except Exception as e:
            logger.error(f"æ›´æ–°è‚¡æŒ‡æ•°æ®å¤±è´¥: {e}")
    
    def _update_sectors(self):
        """æ›´æ–°æ¿å—æ•°æ®"""
        try:
            new_sectors = self._fetch_sector_data()
            with self.lock:
                self.sectors = new_sectors
            logger.info(f"å·²æ›´æ–°{len(new_sectors)}ä¸ªæ¿å—æ•°æ®")
        except Exception as e:
            logger.error(f"æ›´æ–°æ¿å—æ•°æ®å¤±è´¥: {e}")
    
    def _update_telegraph(self):
        """æ›´æ–°ç”µæŠ¥æ•°æ® - æ¯30ç§’å®Œå…¨æ›¿æ¢ä¸ºæœ€æ–°5æ¡"""
        try:
            current_time = datetime.now().strftime("%H:%M:%S")
            logger.info(f"å¼€å§‹æ›´æ–°ç”µæŠ¥æ•°æ® - å½“å‰æ—¶é—´: {current_time}")
            
            new_telegraph = self._fetch_recent_telegraph()
            with self.lock:
                # å®Œå…¨æ›¿æ¢æ± å­å†…å®¹ä¸ºæœ€æ–°çš„5æ¡ç”µæŠ¥
                self.telegraph_items = new_telegraph[:5]
                
            # è®°å½•æ›´æ–°åçš„ç”µæŠ¥æ—¶é—´
            times = [item.news_time for item in self.telegraph_items if item.news_time]
            logger.info(f"å·²æ›´æ–°ç”µæŠ¥æ•°æ®ï¼Œå®Œå…¨æ›¿æ¢ä¸ºæœ€æ–°{len(self.telegraph_items)}æ¡ç”µæŠ¥")
            logger.info(f"æ›´æ–°åç”µæŠ¥æ—¶é—´: {times}")
        except Exception as e:
            logger.error(f"æ›´æ–°ç”µæŠ¥æ•°æ®å¤±è´¥: {e}")
    
    def _fetch_stock_indices(self) -> List[StockIndex]:
        """è·å–è‚¡æŒ‡æ•°æ® - ä»æ–°æµªè´¢ç»API"""
        indices = []
        
        # å®šä¹‰éœ€è¦è·å–çš„è‚¡æŒ‡ä»£ç 
        index_codes = {
            'sh000001': 'ä¸Šè¯æŒ‡æ•°',
            'sz399001': 'æ·±è¯æˆæŒ‡', 
            'sz399006': 'åˆ›ä¸šæ¿æŒ‡',
            'sh000688': 'ç§‘åˆ›50',
            'bj899050': 'åŒ—è¯50'
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://finance.sina.com.cn/'
        }
        
        try:
            # æ„å»ºAPI URL
            codes_param = ','.join(index_codes.keys())
            api_url = f"https://hq.sinajs.cn/list={codes_param}"
            
            response = requests.get(api_url, headers=headers, timeout=10)
            response.encoding = 'gbk'  # æ–°æµªAPIè¿”å›GBKç¼–ç 
            
            if response.status_code == 200:
                lines = response.text.strip().split('\n')
                for line in lines:
                    if '=' in line and '"' in line:
                        # è§£ææ•°æ®æ ¼å¼: var hq_str_sh000001="ä¸Šè¯æŒ‡æ•°,3234.12,3245.67,..."
                        code = line.split('=')[0].replace('var hq_str_', '')
                        if code in index_codes:
                            data_str = line.split('"')[1]
                            if data_str:
                                fields = data_str.split(',')
                                if len(fields) >= 4:
                                    try:
                                        current_price = float(fields[3])
                                        prev_close = float(fields[2])
                                        change = current_price - prev_close
                                        change_percent = (change / prev_close) * 100 if prev_close != 0 else 0
                                        
                                        index = StockIndex(
                                            name=index_codes[code],
                                            code=code,
                                            current_price=current_price,
                                            change=change,
                                            change_percent=change_percent
                                        )
                                        indices.append(index)
                                    except (ValueError, IndexError):
                                        continue
        except Exception as e:
            logger.error(f"è·å–è‚¡æŒ‡æ•°æ®å¤±è´¥: {e}")
        
        return indices
    
    def _fetch_sector_data(self) -> List[SectorData]:
        """è·å–æ¿å—æ•°æ®"""
        sectors = []
        
        try:
            # ä»ä¸œæ–¹è´¢å¯Œè·å–æ¿å—æ•°æ®
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            # æ¿å—æ’è¡ŒAPI
            api_url = "http://push2.eastmoney.com/api/qt/clist/get"
            params = {
                'pn': '1',
                'pz': '50',
                'po': '1',
                'np': '1',
                'ut': 'bd1d9ddb04089700cf9c27f6f7426281',
                'fltt': '2',
                'invt': '2',
                'fid': 'f3',  # æŒ‰æ¶¨è·Œå¹…æ’åº
                'fs': 'm:90+t:2',  # æ¿å—åˆ†ç±»
                'fields': 'f1,f2,f3,f4,f12,f14'
            }
            
            response = requests.get(api_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if data.get('data') and data['data'].get('diff'):
                    items = data['data']['diff']
                    
                    # æŒ‰æ¶¨è·Œå¹…æ’åº
                    items.sort(key=lambda x: float(x.get('f3', 0)), reverse=True)
                    
                    # å–å‰5æ¶¨å¹…
                    for item in items[:5]:
                        if float(item.get('f3', 0)) > 0:
                            sector = SectorData(
                                name=item.get('f14', 'æœªçŸ¥æ¿å—'),
                                change_percent=float(item.get('f3', 0)),
                                sector_type="gainer"
                            )
                            sectors.append(sector)
                    
                    # å–å3è·Œå¹…
                    for item in items[-3:]:
                        if float(item.get('f3', 0)) < 0:
                            sector = SectorData(
                                name=item.get('f14', 'æœªçŸ¥æ¿å—'),
                                change_percent=float(item.get('f3', 0)),
                                sector_type="loser"
                            )
                            sectors.append(sector)
                            
        except Exception as e:
            logger.error(f"è·å–æ¿å—æ•°æ®å¤±è´¥: {e}")
        
        return sectors
    
    def _fetch_recent_telegraph(self) -> List[NewsItem]:
        """è·å–æœ€è¿‘30åˆ†é’Ÿçš„è´¢è”ç¤¾ç”µæŠ¥"""
        telegraph_items = []
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get('https://www.cls.cn/telegraph', headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾ç”µæŠ¥å†…å®¹ - å°è¯•å¤šç§é€‰æ‹©å™¨
            telegraph_blocks = soup.find_all('div', class_=lambda x: x and 'telegraph-content-box' in x)
            
            # å¦‚æœç¬¬ä¸€ä¸ªé€‰æ‹©å™¨æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not telegraph_blocks:
                logger.info("ä½¿ç”¨å¤‡é€‰é€‰æ‹©å™¨æŸ¥æ‰¾ç”µæŠ¥å†…å®¹")
                telegraph_blocks = soup.find_all('div', class_=lambda x: x and 'telegraph' in str(x).lower())
                
            if not telegraph_blocks:
                # å°è¯•æŸ¥æ‰¾åŒ…å«æ—¶é—´æ ¼å¼çš„å†…å®¹å—
                telegraph_blocks = soup.find_all('div', string=lambda text: text and ':' in text and len(text.split(':')) >= 3)
            logger.info(f"æ‰¾åˆ° {len(telegraph_blocks)} ä¸ªç”µæŠ¥å—")
            
            fifteen_minutes_ago = datetime.now() - timedelta(minutes=15)
            
            for i, block in enumerate(telegraph_blocks[:20]):  # é™åˆ¶å¤„ç†æ•°é‡
                try:
                    # æå–æ—¶é—´
                    time_element = block.find('span', class_='telegraph-time-box')
                    if time_element:
                        time_text = time_element.get_text(strip=True)
                        logger.info(f"ç”µæŠ¥å— {i}: æ—¶é—´ {time_text}")
                        # è§£ææ—¶é—´æ ¼å¼ HH:MM:SS
                        if re.match(r'^\d{1,2}:\d{2}:\d{2}$', time_text):
                            today = datetime.now().date()
                            time_parts = time_text.split(':')
                            news_time = datetime.combine(
                                today, 
                                datetime.min.time().replace(
                                    hour=int(time_parts[0]), 
                                    minute=int(time_parts[1]), 
                                    second=int(time_parts[2])
                                )
                            )
                            
                            logger.info(f"è§£ææ—¶é—´: {news_time}, 15åˆ†é’Ÿå‰: {fifteen_minutes_ago}")
                            # åªè¦æœ€è¿‘15åˆ†é’Ÿçš„
                            if news_time < fifteen_minutes_ago:
                                logger.info(f"è·³è¿‡æ—§æ–°é—»: {time_text}")
                                continue
                        else:
                            logger.warning(f"æ—¶é—´æ ¼å¼ä¸åŒ¹é…: {time_text}")
                            continue
                    else:
                        logger.warning(f"ç”µæŠ¥å— {i}: æœªæ‰¾åˆ°æ—¶é—´å…ƒç´ ")
                        continue
                    
                    # æå–æ ‡é¢˜å’Œå†…å®¹
                    content_element = block.find('div')
                    if content_element:
                        title = content_element.get_text(strip=True)
                        # å®‰å…¨æ¸…ç†æ¢è¡Œç¬¦å’Œæ§åˆ¶å­—ç¬¦ï¼Œä¿æŠ¤æ•°å­—å†…å®¹
                        original_title = title  # ä¿å­˜åŸå§‹æ ‡é¢˜ç”¨äºè°ƒè¯•
                        title = title.replace('\n', ' ').replace('\r', ' ').replace('\u2028', ' ').replace('\u2029', ' ')
                        # ä½¿ç”¨æ›´å®‰å…¨çš„ç©ºæ ¼å¤„ç†ï¼Œç¡®ä¿æ•°å­—ä¸è¢«è¯¯åˆ 
                        import re
                        title = re.sub(r'\s+', ' ', title).strip()  # å°†å¤šä¸ªç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
                        
                        # è°ƒè¯•ï¼šè®°å½•æ–‡æœ¬å¤„ç†è¿‡ç¨‹
                        if original_title != title:
                            logger.debug(f"æ–‡æœ¬å¤„ç†: åŸå§‹='{original_title[:50]}...' -> å¤„ç†å='{title[:50]}...'")
                        else:
                            logger.debug(f"æ–‡æœ¬æ— éœ€å¤„ç†: '{title[:50]}...'")
                        if title and len(title) > 10:
                            # æå–é“¾æ¥
                            link_element = block.find('a')
                            url = link_element.get('href', '') if link_element else ''
                            
                            if url and not url.startswith('http'):
                                url = f"https://www.cls.cn{url}"
                            
                            # æå–è‚¡ç¥¨ä¿¡æ¯
                            stock_container = block.find_next('div', class_='industry-stock')
                            stock_info = None
                            if stock_container:
                                stock_items = []
                                stock_links = stock_container.find_all('a')
                                for link in stock_links[:3]:  # æœ€å¤š3åªè‚¡ç¥¨
                                    name_span = link.find('span', class_='c-222')
                                    change_span = link.find('span', class_='c-de0422')
                                    if name_span and change_span:
                                        name = name_span.get_text(strip=True)
                                        change = change_span.get_text(strip=True)
                                        stock_items.append(f"{name} {change}")
                                
                                if stock_items:
                                    stock_info = ' '.join(stock_items)
                            
                            news_item = NewsItem(
                                title=title,
                                url=url,
                                source="è´¢è”ç¤¾ç”µæŠ¥",
                                news_time=time_text if time_element else None,
                                stock_info=stock_info
                            )
                            telegraph_items.append(news_item)
                            logger.info(f"æ·»åŠ ç”µæŠ¥é¡¹: {time_text} - {title[:50]}...")
                            
                except Exception as e:
                    logger.debug(f"è§£æç”µæŠ¥é¡¹é”™è¯¯: {e}")
                    continue
            
            # æŒ‰æ–°é—»æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            def parse_news_time(item):
                if item.news_time:
                    try:
                        time_parts = item.news_time.split(':')
                        today = datetime.now().date()
                        return datetime.combine(today, datetime.min.time().replace(
                            hour=int(time_parts[0]), 
                            minute=int(time_parts[1]), 
                            second=int(time_parts[2])
                        ))
                    except:
                        return datetime.min
                return datetime.min
            
            telegraph_items.sort(key=parse_news_time, reverse=True)
            logger.info(f"æœ€ç»ˆè·å¾— {len(telegraph_items)} ä¸ªæœ‰æ•ˆç”µæŠ¥é¡¹")
            
            # è®°å½•æ’åºåçš„å‰5ä¸ªç”µæŠ¥æ—¶é—´
            for i, item in enumerate(telegraph_items[:5]):
                logger.info(f"æ’åºåç¬¬{i+1}ä¸ªç”µæŠ¥: {item.news_time}")
            
        except Exception as e:
            logger.error(f"è·å–è´¢è”ç¤¾ç”µæŠ¥å¤±è´¥: {e}")
        
        return telegraph_items[:5]  # åªä¿ç•™æœ€æ–°5æ¡
    
    def get_display_content(self) -> Dict:
        """è·å–å½“å‰åº”è¯¥æ˜¾ç¤ºçš„å†…å®¹ - åŸºäº10ç§’è½®æ’­"""
        with self.lock:
            current_time = datetime.now()
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„è½®æ’­æœºåˆ¶ï¼Œå‡å°‘åŒæ­¥é—®é¢˜
            cycle_second = int(current_time.timestamp()) % 10
            
            # è®°å½•è½®æ’­çŠ¶æ€åˆ°è°ƒè¯•æ—¥å¿—
            logger.debug(f"è½®æ’­çŠ¶æ€: å½“å‰æ—¶é—´={current_time.strftime('%H:%M:%S')}, å‘¨æœŸç§’={cycle_second}")
            
            if cycle_second < 5:
                # 0-5ç§’ï¼šæ˜¾ç¤ºç”µæŠ¥ï¼ˆè½®æ’­æœ€æ–°çš„5æ¡ä¸­çš„å‰3æ¡ï¼‰
                if self.telegraph_items:
                    # ä¼˜å…ˆè½®æ’­æœ€æ–°çš„3æ¡ç”µæŠ¥
                    display_telegraph = self.telegraph_items[:3]
                    if display_telegraph:
                        telegraph_index = (cycle_second * len(display_telegraph)) // 5
                        if telegraph_index < len(display_telegraph):
                            return {
                                'type': 'telegraph',
                                'content': display_telegraph[telegraph_index].to_dict()
                            }
                
                return {
                    'type': 'telegraph', 
                    'content': {'title': 'ğŸ“ˆ ç­‰å¾…è´¢è”ç¤¾ç”µæŠ¥æ•°æ®...', 'source': 'è´¢è”ç¤¾ç”µæŠ¥'}
                }
            else:
                # 5-10ç§’ï¼šæ˜¾ç¤ºè‚¡æŒ‡å’Œæ¿å—
                display_data = {
                    'type': 'market',
                    'indices': [idx.to_dict() for idx in self.indices],
                    'sectors': [sector.to_dict() for sector in self.sectors]
                }
                return display_data
    
    def get_indices(self) -> List[Dict]:
        """è·å–è‚¡æŒ‡æ•°æ®"""
        with self.lock:
            return [idx.to_dict() for idx in self.indices]
    
    def get_sectors(self) -> List[Dict]:
        """è·å–æ¿å—æ•°æ®"""
        with self.lock:
            return [sector.to_dict() for sector in self.sectors]
    
    def get_telegraph(self) -> List[Dict]:
        """è·å–ç”µæŠ¥æ•°æ®"""
        with self.lock:
            return [item.to_dict() for item in self.telegraph_items]
    
    def get_status(self) -> Dict:
        """è·å–BigAæ¨¡å¼çŠ¶æ€"""
        with self.lock:
            return {
                'indices_count': len(self.indices),
                'sectors_count': len(self.sectors),
                'telegraph_count': len(self.telegraph_items),
                'last_indices_update': self.last_indices_update.isoformat(),
                'last_sectors_update': self.last_sectors_update.isoformat(),
                'last_telegraph_update': self.last_telegraph_update.isoformat()
            }
    
    def stop(self):
        """åœæ­¢BigAæ•°æ®æ›´æ–°"""
        self.running = False
        if self.update_thread.is_alive():
            self.update_thread.join(timeout=5)

class NewsPool:
    """æ–°é—»æ± ç®¡ç†å™¨"""
    def __init__(self, max_size: int = 100, refresh_interval: int = 60):
        self.news_items: List[NewsItem] = []
        self.max_size = max_size
        self.refresh_interval = refresh_interval
        self.lock = threading.Lock()
        self.last_refresh = None
        self.refresh_thread = None
        self.running = True
        
        # åŠ è½½æ–°é—»æºé…ç½®
        self.load_news_sources_config()
        
        # å¯åŠ¨è‡ªåŠ¨åˆ·æ–°
        self.start_auto_refresh()
        
        # ç«‹å³è·å–ä¸€æ¬¡æ–°é—»
        self.refresh_news()
    
    def load_news_sources_config(self):
        """åŠ è½½æ–°é—»æºé…ç½®ï¼Œæ”¯æŒå¯å¼€å…³é…ç½®"""
        # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
        config_file = os.path.expanduser('~/.claude/news_sources_config.json')
        
        # é»˜è®¤æ–°é—»æºé…ç½®ï¼ˆæ‰€æœ‰æºé»˜è®¤å¯ç”¨ï¼‰
        default_config = {
            'sources': {
                '36kr': {
                    'enabled': True,
                    'name': '36kr',
                    'url': 'https://36kr.com',
                    'selectors': [
                        'a[href*="/p/"]',
                        'a.item-title',
                        '.article-title',
                        'h3.title a'
                    ],
                    'icon': 'ğŸ’¼'
                },
                'techcrunch': {
                    'enabled': True,
                    'name': 'TechCrunch',
                    'url': 'https://techcrunch.com',
                    'selectors': [
                        'h2.post-block__title a',
                        '.post-title a',
                        'h3 a',
                        '.article-title'
                    ],
                    'icon': 'ğŸš€'
                },
                'huxiu': {
                    'enabled': True,
                    'name': 'è™å—…',
                    'url': 'https://www.huxiu.com',
                    'selectors': [
                        '.article-title',
                        'h3.title a',
                        '.news-title',
                        'a.item-title'
                    ],
                    'icon': 'ğŸ¦†'
                },
                'tmtpost': {
                    'enabled': True,
                    'name': 'é’›åª’ä½“',
                    'url': 'https://www.tmtpost.com',
                    'selectors': [
                        'a._tit',
                        '.item ._tit',
                        'h3 a',
                        '.article-title'
                    ],
                    'icon': 'ğŸ”§'
                },
                'leiphone': {
                    'enabled': True,
                    'name': 'é›·é”‹ç½‘',
                    'url': 'https://www.leiphone.com',
                    'selectors': [
                        'h3 a',
                        'a[href*="news"]',
                        '.article-title',
                        '.news-title'
                    ],
                    'icon': 'âš¡'
                },
                'cls_telegraph': {
                    'enabled': True,
                    'name': 'è´¢è”ç¤¾ç”µæŠ¥',
                    'url': 'https://www.cls.cn/telegraph',
                    'selectors': [
                        '.telegraph-content a',
                        'a[href*="/telegraph/"]',
                        '.telegraph-item a',
                        'a'
                    ],
                    'icon': 'ğŸ“ˆ'
                },
                'cls_finance': {
                    'enabled': True,
                    'name': 'è´¢è”ç¤¾ç›˜é¢',
                    'url': 'https://www.cls.cn/subject/1103',
                    'selectors': [
                        'a[href*="/detail/"]',
                        '.article-item a',
                        '.news-item a',
                        'a'
                    ],
                    'icon': 'ğŸ’¹'
                },
                'cls_depth': {
                    'enabled': True,
                    'name': 'è´¢è”ç¤¾æ·±åº¦',
                    'url': 'https://www.cls.cn/depth?id=1000',
                    'selectors': [
                        'a[href*="/depth/"]',
                        '.depth-item a',
                        'h3 a',
                        'a'
                    ],
                    'icon': 'ğŸ“Š'
                }
            }
        }
        
        # å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®
                    for source_key, source_config in default_config['sources'].items():
                        if source_key in user_config.get('sources', {}):
                            # æ›´æ–°ç”¨æˆ·è‡ªå®šä¹‰çš„è®¾ç½®
                            source_config.update(user_config['sources'][source_key])
                    
                    logger.info(f"å·²åŠ è½½ç”¨æˆ·é…ç½®æ–‡ä»¶: {config_file}")
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
                os.makedirs(os.path.dirname(config_file), exist_ok=True)
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, ensure_ascii=False, indent=2)
                logger.info(f"å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_file}")
        except Exception as e:
            logger.warning(f"é…ç½®æ–‡ä»¶å¤„ç†é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # åªå¯ç”¨å·²å¯ç”¨çš„æ–°é—»æº
        self.news_sources = {
            key: config for key, config in default_config['sources'].items()
            if config.get('enabled', True)
        }
        
        enabled_sources = list(self.news_sources.keys())
        logger.info(f"å·²å¯ç”¨çš„æ–°é—»æº: {enabled_sources}")
    
    def start_auto_refresh(self):
        """å¯åŠ¨è‡ªåŠ¨åˆ·æ–°çº¿ç¨‹"""
        if self.refresh_thread is None or not self.refresh_thread.is_alive():
            self.refresh_thread = threading.Thread(target=self._auto_refresh_worker, daemon=True)
            self.refresh_thread.start()
            logger.info("è‡ªåŠ¨åˆ·æ–°çº¿ç¨‹å·²å¯åŠ¨")
    
    def _auto_refresh_worker(self):
        """è‡ªåŠ¨åˆ·æ–°å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                time.sleep(self.refresh_interval)
                if self.running:
                    self.refresh_news()
            except Exception as e:
                logger.error(f"è‡ªåŠ¨åˆ·æ–°é”™è¯¯: {e}")
    
    def refresh_news(self):
        """åˆ·æ–°æ–°é—»æ± """
        logger.info("å¼€å§‹åˆ·æ–°æ–°é—»...")
        
        new_items = []
        for source_key, source_config in self.news_sources.items():
            try:
                items = self._fetch_news_from_source(source_config)
                new_items.extend(items)
                logger.info(f"ä» {source_config['name']} è·å–åˆ° {len(items)} æ¡æ–°é—»")
            except Exception as e:
                logger.error(f"ä» {source_config['name']} è·å–æ–°é—»å¤±è´¥: {e}")
        
        with self.lock:
            # åˆå¹¶æ–°æ—§æ–°é—»ï¼Œå»é‡
            all_items = new_items + self.news_items
            unique_items = []
            seen_titles = set()
            
            for item in all_items:
                if item.title not in seen_titles:
                    seen_titles.add(item.title)
                    unique_items.append(item)
            
            # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            unique_items.sort(key=lambda x: x.timestamp, reverse=True)
            
            # æ¸…ç†è¿‡æœŸå†…å®¹ï¼ˆ6å°æ—¶å‰ï¼‰
            cutoff_time = datetime.now() - timedelta(hours=6)
            fresh_items = [item for item in unique_items if item.timestamp > cutoff_time]
            
            # é™åˆ¶æ± å¤§å°
            self.news_items = fresh_items[:self.max_size]
            self.last_refresh = datetime.now()
            
            logger.info(f"æ–°é—»æ± åˆ·æ–°å®Œæˆï¼Œå½“å‰æœ‰ {len(self.news_items)} æ¡æ–°é—»")
    
    def _fetch_news_from_source(self, source_config: Dict) -> List[NewsItem]:
        """ä»å•ä¸ªæ–°é—»æºè·å–æ–°é—»"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(source_config['url'], headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_items = []
            
            # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
            for selector in source_config['selectors']:
                elements = soup.select(selector)
                if elements:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    break
            
            if not elements:
                logger.warning(f"æœªæ‰¾åˆ°æ–°é—»å…ƒç´ : {source_config['name']}")
                return []
            
            for element in elements[:20]:  # é™åˆ¶æ¯ä¸ªæºæœ€å¤š20æ¡
                try:
                    if element.name == 'a':
                        title = element.get_text(strip=True)
                        url = element.get('href', '')
                    else:
                        link = element.find('a')
                        if link:
                            title = link.get_text(strip=True)
                            url = link.get('href', '')
                        else:
                            title = element.get_text(strip=True)
                            url = ''
                    
                    if title and len(title) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                        # å¤„ç†ç›¸å¯¹URL
                        if url and not url.startswith('http'):
                            if url.startswith('/'):
                                # è·å–åŸºç¡€åŸŸå
                                from urllib.parse import urlparse
                                parsed_url = urlparse(source_config['url'])
                                base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
                                url = f"{base_url}{url}"
                            else:
                                url = f"{source_config['url'].rstrip('/')}/{url}"
                        
                        # å¯¹è´¢è”ç¤¾æ¥æºæå–é¢å¤–ä¿¡æ¯
                        news_time = None
                        stock_info = None
                        
                        if "è´¢è”ç¤¾" in source_config['name'] and url:
                            extracted_data = self._extract_cls_data(url, soup, element)
                            news_time = extracted_data.get('news_time')
                            stock_info = extracted_data.get('stock_info')
                        
                        news_item = NewsItem(title, url, source_config['name'], news_time, stock_info)
                        news_items.append(news_item)
                
                except Exception as e:
                    logger.debug(f"å¤„ç†æ–°é—»é¡¹é”™è¯¯: {e}")
                    continue
            
            return news_items[:20]  # è¿”å›æœ€å¤š20æ¡
            
        except Exception as e:
            logger.error(f"è·å–æ–°é—»å¤±è´¥ {source_config['name']}: {e}")
            return []
    
    def _extract_cls_data(self, url: str, soup: BeautifulSoup, element) -> Dict:
        """æå–è´¢è”ç¤¾ç‰¹å®šæ•°æ®ï¼šæ—¶é—´å’Œè‚¡ç¥¨ä¿¡æ¯"""
        result = {'news_time': None, 'stock_info': None}
        
        try:
            # å¦‚æœæ˜¯è¯¦æƒ…é¡µé¢ï¼Œéœ€è¦è®¿é—®é¡µé¢æå–ä¿¡æ¯
            if '/detail/' in url:
                result = self._fetch_detail_page_data(url)
                
            elif 'telegraph' in url:
                # å¦‚æœæ˜¯ç”µæŠ¥é¡µé¢ï¼Œç›´æ¥ä»å½“å‰é¡µé¢æå–
                result = self._extract_from_telegraph_page(soup, element)
                
        except Exception as e:
            logger.debug(f"æå–CLSæ•°æ®é”™è¯¯: {e}")
        
        return result
    
    def _fetch_detail_page_data(self, url: str) -> Dict:
        """è®¿é—®è¯¦æƒ…é¡µé¢è·å–æ—¶é—´å’Œè‚¡ç¥¨ä¿¡æ¯"""
        result = {'news_time': None, 'stock_info': None}
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            detail_soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ—¶é—´ä¿¡æ¯ - è´¢è”ç¤¾è¯¦æƒ…é¡µçš„æ—¶é—´æ ¼å¼
            # æŸ¥æ‰¾å¯èƒ½çš„æ—¶é—´å…ƒç´ 
            time_selectors = [
                '.time',
                '.publish-time',
                '[class*="time"]',
                '.article-time',
                'time'
            ]
            
            for selector in time_selectors:
                time_element = detail_soup.select_one(selector)
                if time_element:
                    time_text = time_element.get_text(strip=True)
                    if time_text and ('å¹´' in time_text or 'æœˆ' in time_text or ':' in time_text):
                        result['news_time'] = time_text
                        break
            
            # æå–è‚¡ç¥¨ä¿¡æ¯ - æŸ¥æ‰¾è‚¡ç¥¨ç›¸å…³å…ƒç´ 
            stock_selectors = [
                '.industry-stock a',
                '[class*="stock"] a',
                'a[href*="stock"]'
            ]
            
            for selector in stock_selectors:
                stock_elements = detail_soup.select(selector)
                if stock_elements:
                    stock_items = []
                    for elem in stock_elements[:5]:  # æœ€å¤š5åªè‚¡ç¥¨
                        name_span = elem.find('span', class_='c-222') or elem.find('span')
                        change_span = elem.find('span', class_='c-de0422') or elem.find_all('span')[-1] if elem.find_all('span') else None
                        
                        if name_span and change_span:
                            name = name_span.get_text(strip=True)
                            change = change_span.get_text(strip=True)
                            if '+' in change or '-' in change or '%' in change:
                                stock_items.append(f"{name} {change}")
                    
                    if stock_items:
                        result['stock_info'] = ' '.join(stock_items)
                        break
                        
        except Exception as e:
            logger.debug(f"è·å–è¯¦æƒ…é¡µé¢æ•°æ®é”™è¯¯: {e}")
            
        return result
    
    def _extract_from_telegraph_page(self, soup: BeautifulSoup, element) -> Dict:
        """ä»ç”µæŠ¥é¡µé¢æå–æ•°æ®"""
        result = {'news_time': None, 'stock_info': None}
        
        try:
            # æå–æ—¶é—´ä¿¡æ¯
            time_element = soup.find('span', class_='telegraph-time-box')
            if time_element:
                time_text = time_element.get_text(strip=True)
                if time_text and ':' in time_text:
                    # æ ¼å¼åŒ–æ—¶é—´ï¼ˆæ·»åŠ ä»Šå¤©çš„æ—¥æœŸï¼‰
                    today = datetime.now().strftime('%Y-%m-%d')
                    result['news_time'] = f"{today} {time_text}"
            
            # æå–è‚¡ç¥¨ä¿¡æ¯
            stock_container = soup.find('div', class_='industry-stock')
            if stock_container:
                stock_items = []
                stock_links = stock_container.find_all('a')
                for link in stock_links[:5]:  # æœ€å¤š5åªè‚¡ç¥¨
                    name_span = link.find('span', class_='c-222')
                    change_span = link.find('span', class_='c-de0422')
                    if name_span and change_span:
                        name = name_span.get_text(strip=True)
                        change = change_span.get_text(strip=True)
                        stock_items.append(f"{name} {change}")
                
                if stock_items:
                    result['stock_info'] = ' '.join(stock_items)
                    
        except Exception as e:
            logger.debug(f"ä»ç”µæŠ¥é¡µé¢æå–æ•°æ®é”™è¯¯: {e}")
            
        return result
    
    def get_next_news(self) -> Optional[NewsItem]:
        """è·å–ä¸‹ä¸€æ¡æ–°é—»ï¼ˆæ—¶é—´è½®æ’­ï¼‰"""
        with self.lock:
            if not self.news_items:
                return None
            
            # åŸºäºæ—¶é—´çš„ä¼ªéšæœºè½®æ’­
            current_epoch = int(time.time())
            rotation_interval = 5  # 5ç§’è½®æ’­ä¸€æ¬¡
            rotation_index = (current_epoch // rotation_interval) % len(self.news_items)
            
            return self.news_items[rotation_index]
    
    def get_random_news(self, count: int = 5) -> List[NewsItem]:
        """è·å–éšæœºæ–°é—»"""
        with self.lock:
            if not self.news_items:
                return []
            return random.sample(self.news_items, min(count, len(self.news_items)))
    
    def get_status(self) -> Dict:
        """è·å–æœåŠ¡çŠ¶æ€"""
        with self.lock:
            return {
                'total_news': len(self.news_items),
                'last_refresh': self.last_refresh.isoformat() if self.last_refresh else None,
                'sources': list(self.news_sources.keys()),
                'auto_refresh_interval': self.refresh_interval
            }
    
    def stop(self):
        """åœæ­¢æœåŠ¡"""
        self.running = False
        if self.refresh_thread:
            self.refresh_thread.join(timeout=1)

class NewsAPIHandler(BaseHTTPRequestHandler):
    """HTTP APIå¤„ç†å™¨"""
    
    def do_GET(self):
        parsed_path = urllib.parse.urlparse(self.path)
        path = parsed_path.path
        query_params = urllib.parse.parse_qs(parsed_path.query)
        
        try:
            if path == '/status':
                self._handle_status()
            elif path == '/next':
                self._handle_next()
            elif path == '/random':
                count = int(query_params.get('count', ['5'])[0])
                self._handle_random(count)
            elif path == '/refresh':
                self._handle_refresh()
            # BigAæ¨¡å¼APIç«¯ç‚¹
            elif path == '/biga/status':
                self._handle_biga_status()
            elif path == '/biga/next':
                self._handle_biga_next()
            elif path == '/biga/indices':
                self._handle_biga_indices()
            elif path == '/biga/sectors':
                self._handle_biga_sectors()
            elif path == '/biga/telegraph':
                self._handle_biga_telegraph()
            else:
                self._send_error(404, "Not Found")
        except Exception as e:
            logger.error(f"APIé”™è¯¯: {e}")
            self._send_error(500, str(e))
    
    def _handle_status(self):
        status = news_pool.get_status()
        self._send_json_response(status)
    
    def _handle_next(self):
        news_item = news_pool.get_next_news()
        if news_item:
            self._send_json_response(news_item.to_dict())
        else:
            self._send_json_response({'error': 'No news available'})
    
    def _handle_random(self, count: int):
        news_items = news_pool.get_random_news(count)
        response = [item.to_dict() for item in news_items]
        self._send_json_response(response)
    
    def _handle_refresh(self):
        threading.Thread(target=news_pool.refresh_news, daemon=True).start()
        self._send_json_response({'message': 'Refresh started'})
    
    # BigAæ¨¡å¼å¤„ç†å‡½æ•°
    def _handle_biga_status(self):
        """å¤„ç†BigAæ¨¡å¼çŠ¶æ€è¯·æ±‚"""
        status = biga_pool.get_status()
        self._send_json_response(status)
    
    def _handle_biga_next(self):
        """å¤„ç†BigAæ¨¡å¼ä¸‹ä¸€ä¸ªå†…å®¹è¯·æ±‚"""
        content = biga_pool.get_display_content()
        self._send_json_response(content)
    
    def _handle_biga_indices(self):
        """å¤„ç†BigAæ¨¡å¼è‚¡æŒ‡æ•°æ®è¯·æ±‚"""
        indices = biga_pool.get_indices()
        self._send_json_response(indices)
    
    def _handle_biga_sectors(self):
        """å¤„ç†BigAæ¨¡å¼æ¿å—æ•°æ®è¯·æ±‚"""
        sectors = biga_pool.get_sectors()
        self._send_json_response(sectors)
    
    def _handle_biga_telegraph(self):
        """å¤„ç†BigAæ¨¡å¼ç”µæŠ¥æ•°æ®è¯·æ±‚"""
        telegraph = biga_pool.get_telegraph()
        self._send_json_response(telegraph)
    
    def _send_json_response(self, data):
        response = json.dumps(data, ensure_ascii=False, indent=2)
        self.send_response(200)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.send_header('Content-Length', str(len(response.encode('utf-8'))))
        self.end_headers()
        self.wfile.write(response.encode('utf-8'))
    
    def _send_error(self, code: int, message: str):
        self.send_response(code)
        self.send_header('Content-Type', 'text/plain; charset=utf-8')
        self.end_headers()
        self.wfile.write(message.encode('utf-8'))
    
    def log_message(self, format, *args):
        # å‡å°‘HTTPæ—¥å¿—è¾“å‡º
        pass

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    logger.info("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æœåŠ¡...")
    global news_pool, biga_pool, httpd
    if news_pool:
        news_pool.stop()
    if biga_pool:
        biga_pool.stop()
    if httpd:
        httpd.shutdown()
    sys.exit(0)

def check_existing_service():
    """æ£€æŸ¥æ˜¯å¦å·²æœ‰å¥åº·çš„æœåŠ¡åœ¨è¿è¡Œï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡å¯åŠ¨"""
    import subprocess
    try:
        # å…ˆæ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
        result = subprocess.run(['lsof', '-ti:8765'], capture_output=True, text=True, timeout=3)
        if result.returncode == 0 and result.stdout.strip():
            # ç«¯å£è¢«å ç”¨ï¼Œæ£€æŸ¥æœåŠ¡æ˜¯å¦å¥åº·
            try:
                response = requests.get('http://localhost:8765/status', timeout=3)
                if response.status_code == 200:
                    data = response.json()
                    if 'total_news' in data:
                        logger.info(f"æ£€æµ‹åˆ°å¥åº·çš„æ–°é—»æœåŠ¡æ­£åœ¨è¿è¡Œ (æ–°é—»æ•°: {data.get('total_news', 0)})ï¼Œè·³è¿‡å¯åŠ¨")
                        sys.exit(0)
                    else:
                        logger.warning("æœåŠ¡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œéœ€è¦é‡å¯")
                else:
                    logger.warning(f"æœåŠ¡å“åº”å¼‚å¸¸ (çŠ¶æ€ç : {response.status_code})ï¼Œéœ€è¦é‡å¯")
            except Exception as e:
                logger.warning(f"æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {e}ï¼Œéœ€è¦é‡å¯")
            
            # æœåŠ¡ä¸å¥åº·ï¼Œæ¸…ç†è¿›ç¨‹
            pids = result.stdout.strip().split('\n')
            logger.info("æ£€æµ‹åˆ°ç«¯å£è¢«å ç”¨ä½†æœåŠ¡ä¸å¥åº·ï¼Œæ­£åœ¨æ¸…ç†...")
            for pid in pids:
                if pid.strip():
                    try:
                        logger.info(f"æ­£åœ¨åœæ­¢è¿›ç¨‹ PID: {pid}")
                        subprocess.run(['kill', '-9', pid.strip()], timeout=3)
                    except Exception as e:
                        logger.warning(f"åœæ­¢è¿›ç¨‹ {pid} æ—¶å‡ºé”™: {e}")
            
            # ç­‰å¾…ç«¯å£é‡Šæ”¾
            import time
            time.sleep(2)
            logger.info("è¿›ç¨‹å·²æ¸…ç†ï¼Œå‡†å¤‡å¯åŠ¨æ–°æœåŠ¡")
        else:
            logger.info("ç«¯å£8765ç©ºé—²ï¼Œå‡†å¤‡å¯åŠ¨æœåŠ¡")
                
    except Exception as e:
        logger.warning(f"æ£€æŸ¥ç°æœ‰æœåŠ¡æ—¶å‡ºé”™: {e}")
        pass

def main():
    global news_pool, httpd
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœåŠ¡è¿è¡Œ
    check_existing_service()
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        # åˆå§‹åŒ–æ–°é—»æ± 
        logger.info("åˆå§‹åŒ–æ–°é—»æ± ...")
        news_pool = NewsPool()
        
        # åˆå§‹åŒ–BigAæ¨¡å¼æ•°æ®æ± 
        logger.info("åˆå§‹åŒ–BigAæ¨¡å¼æ•°æ®æ± ...")
        global biga_pool
        biga_pool = BigAPool()
        
        # å¯åŠ¨HTTPæœåŠ¡å™¨
        port = int(os.getenv('NEWS_SERVICE_PORT', '8765'))
        server_address = ('localhost', port)
        httpd = HTTPServer(server_address, NewsAPIHandler)
        
        logger.info(f"æ–°é—»æœåŠ¡å·²å¯åŠ¨åœ¨ http://{server_address[0]}:{server_address[1]}")
        logger.info("API endpoints:")
        logger.info("  GET /status  - æœåŠ¡çŠ¶æ€")
        logger.info("  GET /next    - ä¸‹ä¸€æ¡æ–°é—»")
        logger.info("  GET /random?count=N - éšæœºæ–°é—»")
        logger.info("  GET /refresh - æ‰‹åŠ¨åˆ·æ–°")
        logger.info("BigA Mode endpoints:")
        logger.info("  GET /biga/status    - BigAæ¨¡å¼çŠ¶æ€")
        logger.info("  GET /biga/next      - BigAæ¨¡å¼è½®æ’­å†…å®¹")
        logger.info("  GET /biga/indices   - è‚¡æŒ‡æ•°æ®")
        logger.info("  GET /biga/sectors   - æ¿å—æ•°æ®")
        logger.info("  GET /biga/telegraph - ç”µæŠ¥æ•°æ®")
        
        httpd.serve_forever()
        
    except KeyboardInterrupt:
        signal_handler(signal.SIGINT, None)
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    news_pool = None
    biga_pool = None
    httpd = None
    main()